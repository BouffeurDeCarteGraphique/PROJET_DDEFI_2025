# -*- coding: utf-8 -*-
"""Copie de Midterm ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nYRDGT4nfrTNh0eiTkowwKnbGXYjSw7i

#Partie 1 : Collecte et Pr√©paration des Donn√©es

## a) Collecte des donn√©es

Donn√©es Yahoo Finance
"""

import yfinance as yf
import pandas as pd
import numpy as np
from scipy import stats
from datetime import datetime

# G√©n√©rer la date d'aujourd'hui sous le format '2014-01-01'
today = datetime.today().strftime('%Y-%m-%d')
before5=(datetime.today()-pd.DateOffset(years=5)).strftime('%Y-%m-%d')  ## On prend un historique de 5 ans

sp500_data = yf.download('^GSPC', start=before5, end=today, progress=False)
sp500_data.columns = sp500_data.columns.droplevel(1)

sp500_data = sp500_data[['Adj Close','Volume']]
sp500_data = sp500_data.rename(columns={'Adj Close': 'SP500', 'Volume': 'Volume'})
sp500_data.index = sp500_data.index.strftime('%Y-%m-%d')


VIX_data = yf.download('^VIX', start=before5, end=today, progress=False)
VIX_data.columns = VIX_data.columns.droplevel(1)

VIX_data = VIX_data['Adj Close']
VIX_data = VIX_data.rename('VIX')
VIX_data.index = VIX_data.index.strftime('%Y-%m-%d')

sp500_data

"""Donn√©es FRED ( Federal Reserve Bank of st.Louis)"""

pip install fredapi

from fredapi import Fred
import matplotlib.pyplot as plt
import datetime


fred = Fred(api_key='539eb692aa5fe7f6b14e6ac8891de230')

# Les s√©ries de donn√©es FRED pour les taux d'int√©r√™t des obligations am√©ricaines
series_ids = {
    "3M": "DGS3MO",
    "1Y": "DGS1",
    "5Y": "DGS5",
    "10Y": "DGS10",
    "30Y": "DGS30",
    "Inflation": "CPIAUCSL",
    "GDP": "GDP",
    "Unenployment": "UNRATE"
}


start_date = datetime.datetime(2019, 12, 1)
end_date = datetime.datetime(2024, 12, 1)

data = {}
for maturity, series_id in series_ids.items():
    data[maturity] = fred.get_series(series_id, start_date, end_date)

# Convertir les donn√©es en DataFrame pandas
macro_data = pd.DataFrame(data)

#Pour GDP, Inflation et Unenmployment, de p√©riode mensuelle ou trimestrielle, propager la derni√®re valeur connue
macro_data = macro_data.ffill()

macro_data.head()

"""## b) Pr√©traitement des donn√©es

"""

# Concat√©ner toutes les dataframes
date_format = '%Y-%m-%d'
macro_data.index = pd.to_datetime(macro_data.index).strftime(date_format)

df = pd.concat([sp500_data, VIX_data, macro_data], axis = 1)
df.head()

"""Retirer les NAN"""

# Enlever les valeurs NAN :
print("Nombre de lignes avant : ", df.shape[0])
df_cleaned = df.dropna()
print("Nombre de lignes apr√®s : ", df_cleaned.shape[0])

"""Retirer les valeurs aberrantes"""

def remove_outliers(df):

  '''Retirer les outliers'''

  # Compter le nombre initial de lignes
  initial_rows = len(df)

  # Calcul du Z-scores (Calcul √† quel point la valeur est √©loign√©e de la moyenne (en nombre d'√©cart type))
  z = np.abs(stats.zscore(df))
  # Create a mask for rows where any value has a Z-score greater than 5
  mask = (z > 5).any(axis=1)
  # Retirer les lignes contenant des outliers
  df_no_outliers = df[~mask]

  # Compter le nombre de lignes apr√®s nettoyage
  cleaned_rows = len(df_no_outliers)
  rows_removed = initial_rows - cleaned_rows
  print(f"Outliers supprim√©es : {rows_removed}")

  return df_no_outliers

df_cleaned = remove_outliers(df_cleaned)

df_final = df_cleaned.sort_index(ascending = True)
df_final.tail(6)

"""Calcul des returs normaux"""

df_final['SP500_returns'] = df_final['SP500'].pct_change()
#df_final.drop(columns=['SP500'], inplace=True)  # Supprimer l'ancienne colonne si besoin

"""Calcul des log-returns"""

df_final['SP500_Logreturns']= np.log(df_final['SP500']/df_final['SP500'].shift(1))

df_final = df_final.iloc[1:] # retirer le Nan sur la premi√®re ligne

data_reg = df_final.copy()
data_reg = data_reg.drop(columns = ['SP500_returns'])

df_final.tail(6)

from statsmodels.tsa.stattools import adfuller

def adf_test(timeseries):
    print ('R√©sultats du test de Dickey-Fuller augment√©¬†:')
    dftest = adfuller(timeseries, autolag='AIC')
    dfoutput = pd.Series(dftest[0:4], index=['Statistique du test', 'Valeur¬†p', 'D√©calages utilis√©s', 'Nombre d‚Äôobservations utilis√©es'])
    for key,value in dftest[4].items():
       dfoutput['Valeur critique (%s)' % key] = value
    print (dfoutput)

adf_test(df_final['SP500_returns'])

"""P-value << 0..05, nous rejetons l'hypoth√®se nulle et concluons que la s√©rie est stationnaire.
D√©calages utilis√©s: 23
"""

adf_test(df_final['SP500_Logreturns'])

df_final['SP500_Logreturns'].plot(figsize = (13,2))

"""Pour les log returns: P-value << 0.05 nous rejetons l'hypoth√®se nulle => la s√©rie des log returns est stationnaire. Coh√©rent avec le graphique ci-dessus


"""

import matplotlib.dates as mdates

# Convertir l'index en objets datetime
df_final.index = pd.to_datetime(df_final.index)

fig, ax = plt.subplots(figsize=(12, 3))

ax.plot(df_final.index, df_final['SP500_returns'])
ax.set_title('S√©rie temporelle des rendements du SP500')
ax.set_xlabel('Date')
ax.set_ylabel('Rendements')
ax.grid(True)

# Configurer les dates en abscisse
ax.xaxis.set_major_locator(mdates.MonthLocator(interval=6))
ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))

fig.autofmt_xdate()

plt.show()

from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

plot_acf(df_final['SP500_returns'].dropna(), lags=20)
plt.show()

plot_pacf(df_final['SP500_returns'].dropna(), lags=20)
plt.show()

"""Dans les 2 graphes, seul le 1er lag est significatif. Cela sugg√®re √† premi√®re vu un mod√®le AR(1).

## c) Feature engineering
"""

# Moyennes mobiles
df_final['MA_20'] = df_final['SP500_returns'].rolling(window=20).mean()
df_final['MA_50'] = df_final['SP500_returns'].rolling(window=50).mean()

# RSI
def calculate_rsi(data, period=14):
    delta = data.diff()
    gain = delta.where(delta > 0, 0)
    loss = -delta.where(delta < 0, 0)
    avg_gain = gain.rolling(period).mean()
    avg_loss = loss.rolling(period).mean()
    rs = avg_gain / avg_loss
    rsi = 100 - (100 / (1 + rs))
    return rsi

df_final['RSI'] = calculate_rsi(df_final['SP500_returns'])

# MACD
def calculate_macd(data, short_period=12, long_period=26, signal_period=9):
    short_ema = data.ewm(span=short_period, adjust=False).mean()
    long_ema = data.ewm(span=long_period, adjust=False).mean()
    macd = short_ema - long_ema
    signal = macd.ewm(span=signal_period, adjust=False).mean()
    histogram = macd - signal
    return macd, signal, histogram

df_final['MACD'], df_final['MACD_Signal'], df_final['MACD_Histogram'] = calculate_macd(df_final['SP500_returns'])

df_final.head()

import seaborn as sns

correlation_matrix = df_final.corr()

# HeatMap
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', annot_kws={"size": 6}, fmt=".2f")
plt.show()

"""# Partie 2 : D√©veloppement du Mod√®le Pr√©dictif"""

import sklearn as sk

"""### R√©gression lin√©aire

Nous allons ci-dessous construire un mod√®le de r√©gression lin√©aire afin de pr√©dire les rendements du S&P500 √† partir des autres variables de notre base de donn√©e.
"""

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler


# D√©finir les features (X) et la target (y)
X = data_reg.drop(columns=['SP500_Logreturns'])  # Toutes les colonnes sauf les rendements
y = data_reg['SP500_Logreturns']  # Rendements du S&P 500

# Diviser les donn√©es en jeux d'entra√Ænement et de test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)

# Standardiser les features (important pour les variables macro√©conomiques)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Construire et entra√Æner le mod√®le de r√©gression lin√©aire
model = LinearRegression()
model.fit(X_train_scaled, y_train)

# Faire des pr√©dictions
y_pred = model.predict(X_test_scaled)

# √âvaluer le mod√®le
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("MSE:", mse)
print("R¬≤:", r2)

# Afficher les coefficients du mod√®le
coefficients = pd.DataFrame({
    'Feature': X.columns,
    'Coefficient': model.coef_
}).sort_values(by='Coefficient', ascending=False)

print(coefficients)

"""Le MSE est faible. Cependant, cela ne permet pas de juger r√©ellement notre mod√®le de pr√©diction, car les log-rendements journaliers sont par nature tr√®s petits, et donc les √©carts √† la r√©alit√© √©galement. Le R2 nous permettra d'avoir un avis plus pr√©cis sur notre mod√®le.

R2 = 0.0467, ce qui signifie que seulement 4.67% de la variance de nos rendements est expliqu√©e par nos variables ind√©pendantes. Cela est tr√®s faible.

On peut donc conclure que le mod√®le lin√©aire n'est pas pr√©cis et ne permet pas de pr√©dire efficacement nos rendements. Ce qui est logique, car en finance les rendements sont tr√®s volatiles et ne peuvent pas √™tre d√©crits par des mod√®les lin√©aires simples.

### Random Forest Classifier

Random forest classifier => va pr√©dire si le prix va augmenter ou diminuer = Hausse ou Baisse
"""

## Cr√©ation d'un bool√©en: 0 si Baisse le jour suivant;  1 si Hausse le jour suivant
df_final['Predictor'] = (df_final['SP500_returns'].shift(-1) >= 0).astype(int)

#['Predictor'] = df_final['SP500_returns'].apply(lambda x: 0 if x < 0 else 1)
df_final[['SP500_returns','Predictor','SP500']].head(10)

## Model = random Forest Classifier
model1 = sk.ensemble.RandomForestClassifier(n_estimators=100)

## Definition train/test set
Clean = df_final.dropna() # retirer les dernier NA des MACD, MA_50...
X = Clean[['Volume','VIX','Inflation','GDP','MACD','MA_50','RSI','10Y','3M','30Y']]
Y = Clean['Predictor']
X_train, X_test, Y_train, Y_test = sk.model_selection.train_test_split(X,Y, test_size = 0.2) # Loi de parreto 80 / 20

## Entrainement model1
model1.fit(X_train,Y_train)

## Score de pr√©diction model1
model1.score(X_test,Y_test)
score1 = model1.score(X_test, Y_test)
print(f"Accuracy: {score1}")

"""### Gradient Boosting Classifier"""

## Entrainement model1
model2 = sk.ensemble.GradientBoostingClassifier()
model2.fit(X_train,Y_train)

## Score de pr√©diction model2
score2 = model2.score(X_test, Y_test)
print(f"Accuracy: {score2}")

"""### Gradient Boosting Regressor"""

## on essaye de pr√©dire la valeur du return avec le regressor
## Definition train/test set
Clean = df_final.dropna() # retirer les dernier NA des MACD, MA_50...
X1 = Clean[['Volume','VIX','Inflation','MACD','MA_50','MA_20','RSI']]
Y1 = Clean['SP500_returns']
X1_train, X1_test, Y1_train, Y1_test = sk.model_selection.train_test_split(X1,Y1, test_size = 0.2) # Loi de parreto 80 / 20
model3 = sk.ensemble.GradientBoostingRegressor()
model3.fit(X1_train,Y1_train)
model3.score(X1_test,Y1_test)



## Score des Valeurs pr√©dites qui sont dans le bon sens (hausse/baisse)
fitted_values = model3.predict(X1_test)
df = pd.DataFrame()
df['Real'] = Y1_test
df['Fitted'] = fitted_values
df['Diff'] = df['Fitted']/df['Real'] * 100
df['test'] = df['Diff'].apply(lambda x: 1 if x>0 else 0)
df['test'].sum()/df['test'].shape

df['Zeroaxes'] = df['Diff'].apply(lambda x: 0 if x>0 else 0) # pour avoir la ligne y=0

fig = plt.figure(1, figsize=(30, 10))
plt.scatter(df.index,df['Diff'],s=5)
plt.plot(df.index,df['Zeroaxes'])
plt.ylim(-1000, 1000)
# si le point est au dessus de y=0 la pr√©diction est dans le bon sens (hausse ou baisse), proche de y=100 alors elle est exacte

fig, ax = plt.subplots(figsize=(10, 5))
ax.scatter(df.index, df['Real'], s=10, alpha=0.7, label='Valeurs r√©elles', color='blue')
ax.scatter(df.index, df['Fitted'], s=10, alpha=0.7, label='Valeurs pr√©dites', color='orange')

ax.set_title('Comparaison des valeurs r√©elles et pr√©dites', fontsize=16)
ax.set_xlabel('Ann√©es', fontsize=12)
ax.set_ylabel('Valeurs', fontsize=12)
ax.set_ylim(-0.10, 0.10)
ax.legend(fontsize=12)
ax.grid(True, linestyle='--', alpha=0.5) #grille

plt.tight_layout()
plt.show()

"""# Partie 3 : Analyse des R√©sultats et Interpr√©tation

**Random Forest classifier**
"""

model1.fit(X_train, Y_train)

score1 = model1.score(X_test, Y_test)
print(f"Accuracy (simple score): {score1:.4f}")

Y_pred = model1.predict(X_test)

from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score

accuracy = accuracy_score(Y_test, Y_pred)
print(f"Accuracy: {accuracy:.4f}")

# Matrice de confusion
conf_matrix = confusion_matrix(Y_test, Y_pred)
print("\nConfusion Matrix:")
print(conf_matrix)

# Scores
precision = precision_score(Y_test, Y_pred)
recall = recall_score(Y_test, Y_pred)
f1 = f1_score(Y_test, Y_pred)

print(f"\nPrecision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-Score: {f1:.4f}")

class_report = classification_report(Y_test, Y_pred)
print("\nClassification Report:")
print(class_report)

"""**Performance globale plut√¥t faible:**

L'accuracy de 49.58% est proche du hasard, ce qui indique que le mod√®le a besoin d'am√©liorations.

Le mod√®le semble l√©g√®rement favoris√© pour pr√©dire des hausses (classe 1) au d√©triment des baisses (classe 0).

**Sources potentielles d'erreurs:**

Donn√©es non repr√©sentatives: Les indicateurs utilis√©s peuvent √™tre insuffisants pour diff√©rencier efficacement les classes.     
Corr√©lation des variables: Des corr√©lations entre les variables pourraient limiter la capacit√© du mod√®le.

**Pistes d'am√©lioration**

Ajout de variables explicatives en int√©grant davantage de facteurs macro√©conomiques ou des indicateurs techniques plus sp√©cifiques.

Optimisation des hyperparam√®tres: en ajustant les param√®tres du Random Forest (
ùëõ
estimators
,
max¬†depth
,
min¬†samples¬†split
n
estimators
‚Äã
,max¬†depth,min¬†samples¬†split) pour trouver une meilleure configuration.

Test d'autres mod√®les comme le Gradient Boosting qui pourrait mieux capter les variations des donn√©es.

**Gradient Boosting classifier**
"""

Y_pred_gb = model2.predict(X_test)

accuracy_gb = accuracy_score(Y_test, Y_pred_gb)
print(f"Accuracy: {accuracy_gb:.4f}")

# Matrice de confusion
conf_matrix_gb = confusion_matrix(Y_test, Y_pred_gb)
print("\nConfusion Matrix:")
print(conf_matrix_gb)

# Scores
precision_gb = precision_score(Y_test, Y_pred_gb)
recall_gb = recall_score(Y_test, Y_pred_gb)
f1_gb = f1_score(Y_test, Y_pred_gb)

print(f"\nPrecision: {precision_gb:.4f}")
print(f"Recall: {recall_gb:.4f}")
print(f"F1-Score: {f1_gb:.4f}")

class_report_gb = classification_report(Y_test, Y_pred_gb)
print("\nClassification Report:")
print(class_report_gb)

"""L'accuracy (45.76%) et les scores faibles (F1-score, pr√©cision, rappel) montrent que le mod√®le ne capture pas correctement les patterns des donn√©es.
Le mod√®le a une l√©g√®re pr√©f√©rence pour pr√©dire la hausse (classe 1), mais reste impr√©cis dans les deux cas.

Bien qu‚Äôil y ait un √©quilibre relatif entre les classes (103 baisses vs 133 hausses), le mod√®le semble biais√©.
De plus, la complexit√© des interactions avec des variables explicatives utilis√©es (comme les indicateurs technique et la volatilit√©) pourraient ne pas √™tre suffisantes pour capturer toute la complexit√© des donn√©es.

Les r√©sultats actuels ne semblent pas produire des performances satisfaisantes afin de les exploiter de mani√®re fiable.

**Gradient Boosting regressor**
"""

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

Y1_pred = model3.predict(X1_test)

mae = mean_absolute_error(Y1_test, Y1_pred)
mse = mean_squared_error(Y1_test, Y1_pred)
rmse = np.sqrt(mse)
r2 = r2_score(Y1_test, Y1_pred)

print(f"Mean Absolute Error (MAE): {mae:.4f}")
print(f"Mean Squared Error (MSE): {mse:.4f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")
print(f"R-squared (R¬≤): {r2:.4f}")

"""L'erreur moyenne de 0.0056 sugg√®re que les pr√©dictions du mod√®le sont assez proches des valeurs r√©elles en moyenne, ce qui est un bon r√©sultat avec les donn√©es cibles de faible amplitude.

Pour ce qui est de notre faible valeur du MSE, cela indique que les grandes erreurs sont bien contr√¥l√©es et que le mod√®le est performant globalement.

L'erreur moyenne quadratique est de 0.75%, ce qui est faible pour des retours financiers.

Le R¬≤ indique que 51.08% de la variance des rendements du SP500 est expliqu√©e par le mod√®le. Bien qu'un score sup√©rieur √† 0.5 montre une certaine qualit√© du mod√®le, cela laisse une part importante de la variance inexpliqu√©e (environ 49%).

On peut donc en conclure que ce mod√®le semble bien capter les tendances globales des rendements, mais il ne capture pas compl√®tement la variabilit√© des retours, en partie parce que les donn√©es financi√®res ont tendance √† √™tre assez bruit√©es.

## Strat√©gie: Achat/Vente en fct de la pr√©diction du Gradient Boosting Classifier
"""

# On utilise le Gradient Boosting classifier
# Pas de frais de transaction

data_invest = Clean.loc[X.index]
data_invest['SP500_returns'][0] =0 # on d√©marre √† 0
data_invest['Model'] = model2.predict(X)
data_invest = data_invest[['SP500','SP500_returns','Model','Predictor']]

#Wealth index du SP500
data_invest['wealth_index_SP500'] = 1+data_invest['SP500_returns']
data_invest['wealth_index_SP500'] = data_invest['wealth_index_SP500'].cumprod()*100

#Wealth index de la Strat√©gie utilisant de gradient Boosting
data_invest['Wealth_Strat'] = 1+data_invest['SP500_returns']*data_invest['Model'].shift(1)
data_invest['Wealth_Strat'] = data_invest['Wealth_Strat'].cumprod()*100

data_invest['wealth_index_SP500'].plot()
data_invest['Wealth_Strat'].plot()

# Plot le wealth index du SP500 avec les point rouge si la strat vend et vert sinon

plt.figure(figsize=(70, 30))

# Plot data: Use a condition to assign colors based on 'Model'
plt.scatter(data_invest.index, data_invest['wealth_index_SP500'],
            c=data_invest['Model'].map({1: 'green', 0: 'red'}),
            label='Wealth Index', edgecolors='black', alpha=0.7)

plt.title("Wealth Index of S&P 500 with Model-based Coloring", fontsize=16)
plt.xlabel("Date", fontsize=12)
plt.ylabel("Wealth Index S&P 500", fontsize=12)
plt.xticks(rotation=45)
plt.grid(True)

plt.tight_layout()
plt.show()